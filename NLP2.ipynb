{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWqKJ37y9TSam2oS0sYmzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Franck-k3/Franck-k3/blob/main/NLP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTATION**"
      ],
      "metadata": {
        "id": "ND0zZdbtET6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QZ5LPu_SfWp",
        "outputId": "0f723087-e796-4be6-dbab-67247aed7108"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "HRUoI6jn-Tac"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Fonction pour obtenir l'embedding BERT d'un texte**"
      ],
      "metadata": {
        "id": "37GC8oNyABLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embedding(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()"
      ],
      "metadata": {
        "id": "kCnGLbRi_6yU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **Fonction pour comparer deux réponses d'un étudiant à un résumé généré à partir d'un prompt**"
      ],
      "metadata": {
        "id": "kgeMLBKSARJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compare_two_responses_with_summary(prompt_id, student_id1, student_id2):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "\n",
        "    # Chargement des données\n",
        "    prompts_df = pd.read_csv('prompts_train.csv')\n",
        "    summaries_df = pd.read_csv('summaries_train.csv')\n",
        "\n",
        "    # Trouver le texte du prompt et générer un résumé\n",
        "    prompt_text = prompts_df[prompts_df['prompt_id'] == prompt_id]['prompt_text'].values[0]\n",
        "    prompt_summary = summarizer(prompt_text, max_length=130, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
        "\n",
        "    # Trouver les réponses des étudiants correspondant au prompt_id donné\n",
        "    response1 = summaries_df[(summaries_df['prompt_id'] == prompt_id) & (summaries_df['student_id'] == student_id1)]['text'].values[0]\n",
        "    response2 = summaries_df[(summaries_df['prompt_id'] == prompt_id) & (summaries_df['student_id'] == student_id2)]['text'].values[0]\n",
        "\n",
        "    # Calculer les embeddings BERT pour les réponses et le résumé du prompt\n",
        "    response1_embedding = get_bert_embedding(response1, tokenizer, model)\n",
        "    response2_embedding = get_bert_embedding(response2, tokenizer, model)\n",
        "    prompt_summary_embedding = get_bert_embedding(prompt_summary, tokenizer, model)\n",
        "\n",
        "    # Calculer la similarité cosinus pour chaque réponse avec le résumé du prompt\n",
        "    similarity1 = cosine_similarity(response1_embedding, prompt_summary_embedding)\n",
        "    similarity2 = cosine_similarity(response2_embedding, prompt_summary_embedding)\n",
        "\n",
        "    return similarity1[0][0], similarity2[0][0]\n"
      ],
      "metadata": {
        "id": "23p7jEHe_61f"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exemple d'utilisation**"
      ],
      "metadata": {
        "id": "nmpG00gmAl-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt_id = \"814d6b\"  # Remplacez avec le prompt_id approprié\n",
        "student_id1 = \"000e8c3c7ddb\"  # Remplacez avec l'ID de l'étudiant\n",
        "student_id2 = \"0070c9e7af47\"\n",
        "similarite1, similarite2 = compare_two_responses_with_summary(prompt_id, student_id1, student_id2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geFP_uxP_64l",
        "outputId": "c13f8441-55ea-4404-bc75-b2275054dde7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affichage"
      ],
      "metadata": {
        "id": "i09yFWwgCNex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Similarité entre la première réponse de l'étudiant et le résumé du prompt : {similarite1}\")\n",
        "print(f\"Similarité entre la seconde réponse de l'étudiant et le résumé du prompt : {similarite2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATQdTHI6CtRn",
        "outputId": "89a2cd33-c993-4148-e3ed-af1373a995e5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarité entre la première réponse de l'étudiant et le résumé du prompt : 0.707939863204956\n",
            "Similarité entre la seconde réponse de l'étudiant et le résumé du prompt : 0.8359315395355225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fonction pour comparer la concaténation de deux réponses avec un résumé généré du prompt"
      ],
      "metadata": {
        "id": "slGu-U45ILwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_concatenated_responses_with_summary(prompt_id, student_id1, student_id2):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "\n",
        "    # Chargement des données\n",
        "    prompts_df = pd.read_csv('prompts_train.csv')\n",
        "    summaries_df = pd.read_csv('summaries_train.csv')\n",
        "\n",
        "    # Trouver le texte du prompt et générer un résumé\n",
        "    prompt_text = prompts_df[prompts_df['prompt_id'] == prompt_id]['prompt_text'].values[0]\n",
        "    prompt_summary = summarizer(prompt_text, max_length=130, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
        "\n",
        "    # Trouver les réponses des étudiants correspondant au prompt_id donné\n",
        "    response1 = summaries_df[(summaries_df['prompt_id'] == prompt_id) & (summaries_df['student_id'] == student_id1)]['text'].values[0]\n",
        "    response2 = summaries_df[(summaries_df['prompt_id'] == prompt_id) & (summaries_df['student_id'] == student_id2)]['text'].values[0]\n",
        "\n",
        "    # Concaténer les réponses\n",
        "    concatenated_response = response1 + \" \" + response2\n",
        "\n",
        "    # Calculer les embeddings BERT pour la réponse concaténée et le résumé du prompt\n",
        "    concatenated_response_embedding = get_bert_embedding(concatenated_response, tokenizer, model)\n",
        "    prompt_summary_embedding = get_bert_embedding(prompt_summary, tokenizer, model)\n",
        "\n",
        "    # Calculer la similarité cosinus\n",
        "    similarity = cosine_similarity(concatenated_response_embedding, prompt_summary_embedding)\n",
        "\n",
        "    return similarity[0][0]\n",
        "\n",
        "# Exemple d'utilisation\n",
        "prompt_id = \"814d6b\"  # Remplacez avec le prompt_id approprié\n",
        "student_id1 = \"000e8c3c7ddb\"  # Remplacez avec l'ID de la première réponse de l'étudiant\n",
        "student_id2 = \"0070c9e7af47\"  # Remplacez avec l'ID de la seconde réponse de l'étudiant\n",
        "\n",
        "similarite_concat = compare_concatenated_responses_with_summary(prompt_id, student_id1, student_id2)\n",
        "print(f\"Similarité entre la réponse concaténée des étudiants et le résumé du prompt : {similarite_concat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i6101trIKzP",
        "outputId": "6e23b6b3-e12c-4533-c7ae-bb87ae952d02"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarité entre la réponse concaténée des étudiants et le résumé du prompt : 0.8116200566291809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1df4vyRILKlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_summary_performance(model_summary, reference_summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(model_summary, reference_summary)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "model_summary = \"Votre résumé généré\"\n",
        "reference_summary = \"Résumé de référence\"\n",
        "scores = evaluate_summary_performance(model_summary, reference_summary)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXcrRfYgLKQv",
        "outputId": "1b6732fa-d574-45dd-8225-6b853da0bbe4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}]\n"
          ]
        }
      ]
    }
  ]
}